{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pred import predprob\n",
    "from skimage import io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "paths = sys.path\n",
    "sys.path.append('/home/yuyue/yuyue/Synchronized-BatchNorm-PyTorch-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "#         RandomCrop((512, 512)),\n",
    "        transforms.RandomHorizontalFlip(),   # horizontal flip\n",
    "        transforms.RandomVerticalFlip(),   # vertival flip\n",
    "        transforms.ColorJitter(0.2,0.2,0.2,0.04),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # mean, std\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/data/Pathology/SPIE/training_set/breastpathq/datasets/train_labels.csv') \n",
    "df_val = pd.read_csv('/data/Pathology/SPIE/training_set/breastpathq/datasets/val_labels.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_train = list(df_train.loc[(df_train.y==0)].index)\n",
    "# df_train = df_train.drop(index_train,axis=0)\n",
    "# index_val = list(df_val.loc[(df_val.y==0)].index)\n",
    "# df_val = df_val.drop(index_val,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.append(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(paths, extension, df):\n",
    "    images = []\n",
    "    for p in paths:\n",
    "        if ('.'+extension) in p:\n",
    "            slide = p.split('/')[-1].split('_')[0]\n",
    "            slide = int(slide)\n",
    "            rid = p.split('/')[-1].split('_')[1].split('.')[0]\n",
    "            rid = int(rid)\n",
    "            score = df[(df['slide']==slide) & (df['rid']==rid)]['y'].tolist()[0]\n",
    "            if score > 0.05:\n",
    "                images.append([p, float(score)])\n",
    "    shuffle(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPIE_dataset(data.Dataset):\n",
    "    def __init__(self, dirs, loader, extension, transform=None, train=True):\n",
    "        self.samples = make_dataset(dirs, extension, df)\n",
    "        if len(self.samples) == 0:\n",
    "            raise(RuntimeError(\"no files in %s\" % dirs))\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        self.train=train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        sample = Image.fromarray(sample)\n",
    "        #target = torch.tensor(target).long()\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.train:\n",
    "            return sample, target\n",
    "        else:\n",
    "            return sample, target, path\n",
    "        #print('target:',target)\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = glob(\"/data/Pathology/SPIE/training_set/breastpathq/datasets/train/*.tif\")\n",
    "val_samples = glob(\"/data/Pathology/SPIE/training_set/breastpathq/datasets/validation/*.tif\")\n",
    "#test_samples = torch.load( '/data/AlgProj/ydx/ydx/zhongshan/datapath/20190410_4_cls/test_444.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SPIE_dataset(train_samples, io.imread, 'tif', transform=data_transforms['train'])\n",
    "val_dataset = SPIE_dataset(val_samples, io.imread, 'tif', transform=data_transforms['val'])\n",
    "#test_dataset = Rose_dataset(test_samples, Image.open, 'jpg', transform=data_transforms['test'], train=False)\n",
    "image_datasets = {'train':train_dataset, 'val':val_dataset}\n",
    "dataloaders = {\"train\": torch.utils.data.DataLoader(image_datasets[\"train\"], batch_size=32,\n",
    "                                             shuffle=True, num_workers=16),\n",
    "               \"val\": torch.utils.data.DataLoader(image_datasets[\"val\"], batch_size=8,\n",
    "                                             shuffle=True, num_workers=4)}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    save_dir = \"/data/yuyue/SPIE/model_weight/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    state_dict = model.module.state_dict()\n",
    "    pth = os.path.join(save_dir, \"2_classes_stage2_0626_512.pth\")\n",
    "    torch.save(state_dict, pth)\n",
    "\n",
    "def train_model(model, criterion1,criterion2, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    model = DataParallel(model)\n",
    "   # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100.0\n",
    "    best_epoch = 0\n",
    "              \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_labels = []\n",
    "            epoch_outputs = []\n",
    "            # Iterate over data.\n",
    "#             count  = 0\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                #print('labels.type=',labels.type())\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                outputs = nn.Sigmoid()(outputs[:,0])\n",
    "                #outputs = outputs.unsqueeze(1)\n",
    "                #print(outputs.shape)\n",
    "                outputs_normalize = nn.Softmax(dim=0)(outputs)\n",
    "                label_normalize = nn.Softmax(dim=0)(labels)\n",
    "                log_outputs_normalize = torch.log(outputs_normalize)\n",
    "                \n",
    "                \n",
    "#                 labels_KL = torch.zeros((len(labels),2))\n",
    "#                 labels_KL[:,0] = labels\n",
    "#                 labels_KL[:,1] = 1-labels\n",
    "                \n",
    "                # _, preds = torch.max(outputs.data, 1) # pred值为output中最大值的位置（0是neg,1是pos）\n",
    "                labels = labels.float()                \n",
    "\n",
    "                loss1 = criterion1(outputs, labels)\n",
    "                loss2 = criterion2(log_outputs_normalize, label_normalize.float())\n",
    "                \n",
    "                loss = loss1+10*loss2\n",
    "                #print(\"outputs=\",outputs)\n",
    "                #print(\"labels=\",labels)\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                for n in range(len(labels)):\n",
    "                    #print(labels[n].data.cpu().item())\n",
    "                    pred_value = outputs[n].data.cpu().item()\n",
    "                    epoch_labels.append(labels[n].data.cpu().item())\n",
    "                    epoch_outputs.append(pred_value)\n",
    "                #print(epoch_labels)\n",
    "                # statistics\n",
    "                running_loss += loss.item()* inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "#                 count += 1\n",
    "#                 if count%100 == 0:\n",
    "#                     print(\"batch %d:\" % count)\n",
    "#                     print(phase+\"_loss:\", loss.data[0])\n",
    "#                     print(phase+\"_acc:\", torch.sum(preds == labels.data)/len(labels.data))\n",
    "\n",
    "            p_k = predprob(np.array(epoch_labels),np.array(epoch_outputs))\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(\n",
    "                phase, epoch_loss))\n",
    "            print('p_k = ',p_k)\n",
    "#             if phase == \"train\":\n",
    "#                 save_model(model, epoch)\n",
    "\n",
    "#             # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                save_model(model, epoch)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('Best val epoch: {:4f}'.format(best_epoch))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import DataParallel\n",
    "from densenet import densenet169\n",
    "model_ft = densenet169(pretrained=True)\n",
    "num_ftrs = model_ft.classifier.in_features\n",
    "model_ft.classifier = nn.Linear(num_ftrs, 1)\n",
    "#model_ft.add_module(\"sigmoid\", module=nn.Sigmoid())\n",
    "from sync_batchnorm import convert_model\n",
    "model_ft = convert_model(model_ft)\n",
    "\n",
    "# load pretrained model\n",
    "model_ft.load_state_dict(torch.load(\"/data/yuyue/SPIE/model_weight/2_classes_stage2_0626_256.pth\"))\n",
    "# print(\"pretrained model loaded\")\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.KLDivLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "# do not forget to change learning rate\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9,weight_decay=1e-3)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4,weight_decay=1e-4)\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=10, eta_min=0, last_epoch=-1)\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/torch/nn/functional.py:1906: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0150\n",
      "p_k =  0.8911145339125175\n",
      "val Loss: 0.0181\n",
      "p_k =  0.9039414414414415\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 0.0107\n",
      "p_k =  0.913873965639579\n",
      "val Loss: 0.0179\n",
      "p_k =  0.918581081081081\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.0101\n",
      "p_k =  0.9184448889141617\n",
      "val Loss: 0.0265\n",
      "p_k =  0.8987612612612612\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.0095\n",
      "p_k =  0.9200737413169764\n",
      "val Loss: 0.0190\n",
      "p_k =  0.9065315315315315\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.0088\n",
      "p_k =  0.923834977019707\n",
      "val Loss: 0.0243\n",
      "p_k =  0.9056306306306307\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.0075\n",
      "p_k =  0.931425025403175\n",
      "val Loss: 0.0169\n",
      "p_k =  0.9121621621621622\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.0066\n",
      "p_k =  0.9359535070391312\n",
      "val Loss: 0.0187\n",
      "p_k =  0.9106981981981982\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.0057\n",
      "p_k =  0.9418252871403483\n",
      "val Loss: 0.0165\n",
      "p_k =  0.9099099099099099\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.0057\n",
      "p_k =  0.9437726578602327\n",
      "val Loss: 0.0176\n",
      "p_k =  0.909009009009009\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.0055\n",
      "p_k =  0.9427606514091036\n",
      "val Loss: 0.0178\n",
      "p_k =  0.906981981981982\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.0054\n",
      "p_k =  0.9444146391512991\n",
      "val Loss: 0.0171\n",
      "p_k =  0.9075450450450451\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.0055\n",
      "p_k =  0.9433119806565021\n",
      "val Loss: 0.0162\n",
      "p_k =  0.9087837837837838\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.0053\n",
      "p_k =  0.9446511585743236\n",
      "val Loss: 0.0171\n",
      "p_k =  0.9094594594594594\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.0058\n",
      "p_k =  0.9410522724407073\n",
      "val Loss: 0.0178\n",
      "p_k =  0.9092342342342342\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.0055\n",
      "p_k =  0.9433161012039415\n",
      "val Loss: 0.0178\n",
      "p_k =  0.9036036036036036\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.0062\n",
      "p_k =  0.9376684788834305\n",
      "val Loss: 0.0174\n",
      "p_k =  0.9078828828828829\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.0070\n",
      "p_k =  0.93306747561254\n",
      "val Loss: 0.0192\n",
      "p_k =  0.9077702702702704\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.0064\n",
      "p_k =  0.9365130773814085\n",
      "val Loss: 0.0188\n",
      "p_k =  0.9061936936936936\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.0062\n",
      "p_k =  0.9376717753213821\n",
      "val Loss: 0.0169\n",
      "p_k =  0.9113738738738739\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.0067\n",
      "p_k =  0.9340728891877659\n",
      "val Loss: 0.0219\n",
      "p_k =  0.9136261261261261\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.0073\n",
      "p_k =  0.9285612449327567\n",
      "val Loss: 0.0228\n",
      "p_k =  0.8968468468468469\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.0098\n",
      "p_k =  0.9155213605058714\n",
      "val Loss: 0.0196\n",
      "p_k =  0.8958333333333333\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.0070\n",
      "p_k =  0.9317966987822134\n",
      "val Loss: 0.0206\n",
      "p_k =  0.9063063063063064\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.0053\n",
      "p_k =  0.9445745163919498\n",
      "val Loss: 0.0200\n",
      "p_k =  0.9011261261261261\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.0047\n",
      "p_k =  0.9484305246857876\n",
      "val Loss: 0.0179\n",
      "p_k =  0.9064189189189189\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.0039\n",
      "p_k =  0.9551799813916078\n",
      "val Loss: 0.0190\n",
      "p_k =  0.9041666666666667\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.0036\n",
      "p_k =  0.9571908085420597\n",
      "val Loss: 0.0183\n",
      "p_k =  0.9054054054054055\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.0033\n",
      "p_k =  0.9597175282319308\n",
      "val Loss: 0.0184\n",
      "p_k =  0.9072072072072073\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0029\n",
      "p_k =  0.9629364998916297\n",
      "val Loss: 0.0179\n",
      "p_k =  0.9043918918918918\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0028\n",
      "p_k =  0.9652547198810646\n",
      "val Loss: 0.0181\n",
      "p_k =  0.9058558558558558\n",
      "Training complete in 36m 52s\n",
      "Best val epoch: 11.000000\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion1,criterion2, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
